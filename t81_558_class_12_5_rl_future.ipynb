{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klGNgWREsvQv"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_12_5_rl_future.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmDI-h7cI0tI"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 12: Reinforcement Learning**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsaQlK8fFQqH"
   },
   "source": [
    "# Module 12 Video Material\n",
    "\n",
    "* Part 12.1: Introduction to Introduction to Gymnasium [[Video]](https://www.youtube.com/watch?v=FvuyrpzvwdI&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_1_reinforcement.ipynb)\n",
    "* Part 12.2: Introduction to Q-Learning [[Video]](https://www.youtube.com/watch?v=VKuqvbG_KAw&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_2_qlearningreinforcement.ipynb)\n",
    "* Part 12.3: Stable Baselines Q-Learning [[Video]](https://www.youtube.com/watch?v=kl7zsCjULN0&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_3_pytorch_reinforce.ipynb)\n",
    "* Part 12.4: Atari Games with Stable Baselines Neural Networks [[Video]](https://www.youtube.com/watch?v=maLA1_d4pzQ&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_4_atari.ipynb)\n",
    "* **Part 12.5: Future of Reinforcement Learning** [[Video]](https://www.youtube.com/watch?v=-euo5pTjP8E&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi) [[Notebook]](t81_558_class_12_5_rl_future.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u9QVVsShC9X"
   },
   "source": [
    "# Part 12.5: Future Directions of Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) initially showed great promise as it defeated humans in games such as chess and Go. The 1990s heralded the birth of RL, a time when the field was a conclave of a few passionate souls. In 1995, the first community began to form at the inaugural National Science Foundation workshop on RL. \n",
    "\n",
    "As the new millennium dawned, RL's presence grew silently yet steadfastly, never quite breaking into mainstream machine learning research within machine learning until the advent of DeepMind. Their innovative synthesis of deep learning with RL demonstrated with Atari gaming, was a revelation. It sparked a renaissance, suddenly making RL the object of desire for tech conglomerates and startups, evidenced by Google's princely acquisition of DeepMind.\n",
    "\n",
    "To many, RL appears triumphant, destined to continue its march forward. Sessions dedicated to RL are overflowing at AI and ML conferences, and the influx of papers is ongoing, reinforcing a narrative of success. \n",
    "\n",
    "However, some researchers believe that RL may be at a dead end. When we impose on RL the stringent criteria of learning in real-time, common in the real world, the picture shifts. The current methods of deep RL, which shine in the simulated realities where failure is inconsequential, and repetition is infinite, falter against the complex learning that real life demands. Consider how humans learn to master the art of driving. It's not merely a function of operating the vehicle but a culmination of years of passive observation, an intricate dance of the senses and cognition that no current RL algorithm can claim to replicate. OpenAI, once the leading proponent of RL, has divested itself of the OpenAI Gym.\n",
    "\n",
    "The foray into teaching machines through RL has often ignored this fundamental aspect of human learningâ€”the vast repository of implicit knowledge we bring to each new learning experience. The stark contrast in learning efficiencies between humans and RL algorithms becomes apparent. Where deep RL takes days and millions of iterations to grasp a game like Frostbite, a human requires only a minute and a few hundred trials. This disparity cannot be understated.\n",
    "\n",
    "So, where does this leave RL? Can it be rescued from the potential impasse where current trends lead? Some believe the answer lies not in the abandonment of RL but in its evolution. The future of RL should embrace a model that integrates observation, mimicking, transfer learning, and the scaffolding provided by prior knowledge. This paradigm resonates with how humans acquire complex skills. The musings of Richard Feynman on education resonate deeply with this issue. Despite his unparalleled eloquence in teaching physics, Feynman acknowledged the instruction limitations. This sentiment refers to an older, perhaps more universal understanding of learning: We absorb best what we are already primed to receive.\n",
    "\n",
    "This recognition brings us to a pivotal juncture for RL. The way forward is not to mimic the human learning process but to create a symbiotic framework where machines can benefit from the richness of the human experience. We stand on the brink of a transformative evolution in RL or its decline into obsolescence. The choice is ours to make, the direction ours to steer. The future of RL will depend on whether we choose to learn from the depth of human experience or continue down a path of isolated, knowledge-free computation.\n",
    "\n",
    "For a more indepth analysis of the future of reinforcement learning, consider the Quora post by [Sridhar Mahadevan](https://www.quora.com/Is-reinforcement-learning-a-dead-end).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of t81_558_class_12_05_apply_rl.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
